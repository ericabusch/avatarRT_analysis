# avatarRT_analysis

This repository contains the analysis scripts for our real-time fMRI experiment. `behavioral_learning.py` has the results for the BCI learning effects indexed by behavior (i.e., $\Delta Brain Control$). `neural_EVR_alanysis.py` contains the estimated changes in neural alignment with manifold components as a result of BCI learning. `joystick_decoding` and `searchlight_location_prediction_himalaya.py` contain decoding of the avatar's location in space, both during the joystick task and across neurofeedback runs. `run_randomise.sh` shows how we ran statistical tests over the searchlight results. 

Plots were generated within the jupyter notebooks, though statistics were run within the `BCI_statistics_final.Rmd` script and visualized over the plots with those results from bootstrapping. Linear mixed effects models were also run in `BCI_statistics_final.Rmd`. All python scripts were run within the environment listed in `environment.yml`. 

This software was implemented and run on a Red Hat Linux Compute Cluster ("Red Hat Enterprise Linux Server 7.9 (Maipo)"). The demo was additionally tested on a macOS Ventura version 13.4.1. The conda environment specified in environment.yml contains all dependencies to recreate our compute environment. The environment can be created using conda env create environment.yml with anaconda (tested with conda version 4.10.1 on macOS and conda 4.12.0 on Linux); took around 16 minutes to create environment on local macOS. The scripts here were run with parallelization both within-script and at the job level on our HPC cluster using dead-simple queue.
